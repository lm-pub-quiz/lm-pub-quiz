{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>LM Pub Quiz implements a knowledge probing approach which uses LM's inherent ability to estimate the log-likelihood of any given textual statement. For more information visit the LM Pub Quiz website.</p> <p></p> <p>Illustration of how LM Pub Quiz evaluates LMs: Answers are ranked by the (pseudo) log-likelihoods of the textual statements derived from all of the answer options.</p> <p>The following sections give a quick overview how to calculate the BEAR-score for a given model. For a more detailed look into the results, please take a look at the example workflow.</p>"},{"location":"#installing-the-package","title":"Installing the Package","text":"<p>To install the package from PyPI, simply run:</p> <pre><code>pip install lm-pub-quiz\n</code></pre> <p>Note</p> <p>For alternative setups (esp. for contributing to the library), see the development section.</p>"},{"location":"#evaluating-a-model","title":"Evaluating a Model","text":"<p>Models can be loaded and evaluated using the <code>Evaluator</code> class. First, create an evaluator for the model, then run <code>evaluate_dataset</code> with the loaded dataset.</p> <pre><code>from lm_pub_quiz import Dataset, Evaluator\n\n# Load the dataset\ndataset = Dataset.from_name(\"BEAR\")\n\n# Load the model\nevaluator = Evaluator.from_model(\n    \"gpt2\",\n    model_type=\"CLM\",\n)\n# Run the evaluation and save the\nresults = evaluator.evaluate_dataset(\n    dataset,\n    template_index=0,\n    save_path=\"gpt2_results\",\n    batch_size=32,\n)\n</code></pre>"},{"location":"#assessing-the-results","title":"Assessing the Results","text":"<p>To load the results and compute the overall accuracy, you can use the following lines of code:</p> <pre><code>from lm_pub_quiz import DatasetResults\n\nresults = DatasetResults.from_path(\"gpt2_results\")\n\nprint(results.get_metrics(\"accuracy\"))\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>You can use the API to call the evaluation from a python script. For this, you need to load a dataset (see Data Files for how these should be structured) and then execute the evaluation function using your desired configuration.</p> Example<pre><code>from lm_pub_quiz import Dataset, Evaluator\n\n# Load dataset\ndataset = Dataset.from_name(\"BEAR\")\n\n# Create Evaluator (and load model)\nevaluator = Evaluator.from_model(\"distilbert-base-cased\")\n\n# Run evaluation\nresult = evaluator.evaluate_dataset(dataset)\n\n# Save result object\nresult.save(\"outputs/my_results\")\n</code></pre>"},{"location":"data_files/","title":"Structure of the Data Files","text":"<p>A project dataset comprises a series of <code>.jsonl</code> files and a single <code>.json</code> metadata file. These files collectively encapsulate relational knowledge relations. TREx dataset is included for demonstration purposes.</p>"},{"location":"data_files/#relation-data-jsonl-files","title":"Relation data: JSONL Files","text":"<p>Each <code>.jsonl</code> file represents relational data points (triple). The format for each line in these files consists of:</p> <ul> <li>predicate_id: A unique identifier (Wikidata) for the relation.</li> <li>sub_id: A unique identifier (Wikidata) for the subject entity.</li> <li>sub_label: A readable label or name for the subject entity.</li> <li>obj_id: A unique identifier (Wikidata) for the object entity.</li> <li>obj_label: A readable label or name for the object entity.</li> </ul>"},{"location":"data_files/#example-instance","title":"Example Instance:","text":"<pre><code>{\n  \"predicate_id\":\"P30\",\n  \"sub_id\":\"Q3108669\",\n  \"sub_label\":\"Lavoisier Island\",\n  \"obj_id\":\"Q51\",\n  \"obj_label\":\"Antarctica\"\n}\n</code></pre>"},{"location":"data_files/#metadata-json-file","title":"Metadata JSON File","text":"<p>The <code>metadata_relations.json</code> file provides contextual and auxiliary information about each relation (identified by the <code>predicate_id</code>) available in the <code>.jsonl</code> files. It specifies:</p> <ul> <li>templates: A list of sentence templates that can be used to form human-readable statements from triples in a given relation.</li> <li>answer_space: A list of possible answers that can be associated with the particular relation (set of all answers).</li> </ul>"},{"location":"data_files/#example-metadata","title":"Example Metadata:","text":"<pre><code>{\n  \"P30\": {\n    \"templates\": [\"[X] is located in [Y].\"],\n    \"answer_space\": [\"Asia\", \"Antarctica\", \"Africa\", \"Europe\", \"Americas\", \"Oceania\"]\n  }\n}\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>If you want to make changes to the package (in order to contribute or customize the experiments), we recommend cloning it and using the environments automatically created by <code>hatch</code>.</p>"},{"location":"development/#using-hatch-managed-environment-recommended-for-development","title":"Using hatch-managed environment (Recommended for Development)","text":"<p>If you want to contribute to lm-pub-quiz, we recommend to use hatch. In this case you need to:</p> <ol> <li>Install hatch (if you haven't already), and</li> <li>clone the repository: <code>git clone git@github.com:lm-pub-quiz/lm-pub-quiz.git</code></li> </ol> <p>In the cloned directory, you can now run the relevant commands in a hatch shell. You can either run a command with <code>hatch run &lt;command&gt;</code> or run <code>hatch shell</code> and continue to work within the activated environment.</p> <p>This allows you to run the test cases (<code>hatch run test</code>), format the code (<code>hatch run lint:fmt</code>), and check for typing inconsistencies (<code>hatch run lint:all</code>). </p> <p>By specifying the environment before the command (as in <code>lint:</code>), commands can be run in specific environment that hatch manages for you. By running <code>hatch run all:test</code> the library can be tested on multiple python versions.</p> <p>Use <code>hatch run serve-docs</code> to start a local web server serving the current state of this documentation.</p> <p>For more information on the usage of hatch, we refer to the documentation of hatch.</p>"},{"location":"development/#without-hatch","title":"Without hatch","text":"<p>Alternatively, you can clone the repository, then (in your desired environment) and run:</p> <pre><code># replace lm-pub-quiz with the path to the repository\npip install -e lm-pub-quiz\n</code></pre> <p>This allows you to make changes source code which will be reflected directly within your manually managed environment.</p>"},{"location":"example/","title":"Example Workflow","text":"<p>In this short guide we will go through a brief workflow example, in which we run the BEAR probe on the gpt2 model and look at some results using the python api.</p> <p>For additional examples, see the <code>examples/</code> directory.</p>"},{"location":"example/#run-bear-probe-on-a-given-model","title":"Run BEAR probe on a given model","text":"<p>First we run the BEAR probe on the given model and save its results to our file system.</p> <pre><code>from lm_pub_quiz import Dataset, Evaluator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the BEAR dataset\ndataset = Dataset.from_name(\"BEAR\")\n\n# Load the gpt2 model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n\n\n# Run the BEAR evaluator and save the results\nresult_save_path = \"&lt;BEAR results save path&gt;\"  # doctest: +SKIP\n\n# In case the tokenizer cannot be loaded from the model directory, it may be loaded explicitly and passed to the Evaluator.from_model method via the 'tokenizer=' keyword\nevaluator = Evaluator.from_model(model, model_type=\"CLM\", device=\"cuda:0\")\nevaluator.evaluate_dataset(dataset, save_path=result_save_path, batch_size=32)\n</code></pre>"},{"location":"example/#inspect-bear-probe-results","title":"Inspect BEAR Probe results","text":"<p>We can load the BEAR probe results as follows:</p> <pre><code>from lm_pub_quiz import DatasetResults\nbear_results = DatasetResults.from_path(result_save_path)\n</code></pre>"},{"location":"example/#aggregate-results","title":"Aggregate Results","text":"<p>The DatasetResults object allows us to retrieve some aggregate results. Here we are loading the accuracy and the precision_at_k metrics: <pre><code>metrics = bear_results.get_metrics([\"accuracy\", \"num_instances\"], accumulate=False)\n</code></pre> The method returns a pandas dataframe that holds the specified metrics for each relation (P6 to P7959) in the BEAR dataset (here showing the first five entries): <pre><code>     accuracy  num_instances relation_type\nP6   0.183333             60          None\nP19  0.206667            150          None\nP20  0.160000            150          None\nP26  0.050000             60          None\nP27  0.406667            150          None\n</code></pre></p> <p>To accumulate the accuracy for all relations, simply use:</p> <pre><code>print(bear_results.get_metrics(\"accuracy\"))\n# 0.1495\n</code></pre> <p>For the gpt2 model we thus get an accuracy of <code>0.1495</code>. Note that this overall accuracy score is based only on the first template of each relation, which is the template considered by default by the <code>Evaluator</code>.</p>"},{"location":"example/#individual-results","title":"Individual Results","text":"<p>The DatasetResults object holds <code>RelationResult</code> objects for each relation in the probe that can be accessed using the relation codes in a key-like manner. If we want to take a more detailed look at the results for individual relations we may look at the instance tables these RelationResults hold:</p> <pre><code>relation_instance_table = bear_results[\"P36\"].instance_table\nprint(relation_instance_table.head())\n</code></pre> <p><pre><code>      sub_id                  sub_label  answer_idx                                         pll_scores  obj_id      obj_label\n3      Q1356                West Bengal           0  [-28.071779251, -35.064821243299996, -32.31778...   Q1348        Kolkata\n11     Q1028                    Morocco           1  [-33.614648819, -26.9230899811, -32.1363086701...   Q3551          Rabat\n15  Q3177715         Pagaruyung Kingdom           2  [-65.55403518690001, -67.46153640760001, -66.3...   Q3492        Sumatra\n18   Q483599  Southern Federal District           3  [-46.7988452912, -49.6077213287, -49.030160904...    Q908  Rostov-on-Don\n20    Q43684                      Henan           4  [-36.29014015210001, -37.7681064606, -41.59478...  Q30340      Zhengzhou\n</code></pre> Here we see the instance table for the relation <code>P36</code>. Each row of this instance table holds the results for a specific instance of this relation, i.e. the log-likelihood scores of instantiations of a template of the relation with the subject of this row and the objects in the relation answer space. The columns can be interpreted as follows:</p> <ul> <li><code>sub_id</code>: wikidata code of the subject instance of this row</li> <li><code>sub_label</code>: label of that subject instance</li> <li><code>sub_aliases</code>: alternative labels for that subject instance</li> <li><code>answer_idx</code>: id in the pll_scores list for the score of the true answer for this instance</li> <li><code>pll_scores</code>: (pseudo) log-likelihood scores for all objects in the answer space.</li> <li><code>obj_id</code>: wikidata code for the true object in the answer space</li> <li><code>obj_label</code>: label for the true object in the answer space</li> </ul> <p>Note that the <code>pll_scores</code> are ordered corresponding to the orders of the objects in this relations answer space (<code>bear_results[\"P36\"].answer_space</code>).</p> <p>We will lastly be looking at two examples of what we can do with this data: (1) Collect the specific instances the model got right for each relation. (2) Estimate the prior for each object in the answer space for each relation.</p>"},{"location":"example/#correct-instances","title":"Correct Instances","text":"<p>To gain more insight into the individual strengths and weaknesses of the model under investigation, we may want to inspect which specific instances of a relation the model got right and where it was wrong. Given the instance table this information is easy to retrieve. We only need to compare the index of the greatest pll_score to the <code>answer_idx</code> to determine whether for a given subject the correct object was scored as most likely:</p> <pre><code>relation_instance_table[\"correctly_predicted\"] = relation_instance_table.apply(lambda row: row.answer_idx == np.argmax(row.pll_scores), axis=1)\n</code></pre> <pre><code>      sub_id                  sub_label  answer_idx                                         pll_scores  obj_id      obj_label  correctly_predicted\n3      Q1356                West Bengal           0  [-28.071779251, -35.064821243299996, -32.31778...   Q1348        Kolkata                False\n11     Q1028                    Morocco           1  [-33.614648819, -26.9230899811, -32.1363086701...   Q3551          Rabat                 True\n15  Q3177715         Pagaruyung Kingdom           2  [-65.55403518690001, -67.46153640760001, -66.3...   Q3492        Sumatra                False\n18   Q483599  Southern Federal District           3  [-46.7988452912, -49.6077213287, -49.030160904...    Q908  Rostov-on-Don                False\n20    Q43684                      Henan           4  [-36.29014015210001, -37.7681064606, -41.59478...  Q30340      Zhengzhou                False\n</code></pre>"},{"location":"example/#answer-space-priors","title":"Answer Space Priors","text":"<p>Another question we may ask ourselves is to what extent a models log-likelihood scores for individual instantiations of a relation depend on what the model has learned about the connection between the specific subject and object or to what extent these scores are determined by a general bias the model possess towards certain objects in the answer space.</p> <p>To address this we may want to estimate the priors for all objects in the answer space.</p> <p>For a causal language model such as gpt2 the <code>pll_scores</code> are identical to the log-likelihood of the sentences derived by instantiating the template with the given subjects and objects. Taking the relation P30 as an example, with the template <code>[X] is located in [Y].</code>, the subject <code>Nile</code> and the object <code>Africa</code>, the <code>pll_score</code>for this pairing is the log of the probability assigned by the evaluated language model to the sentence <code>Nile is located in Africa</code>.</p> <p>Calculating the softmax over the <code>pll_scores</code> for a given subject of a relation gives us the conditional probabilities of the instantiated sentences of the relation conditioned on the fact that one of the instantiations is correct.</p> <p>Averaging these probability distributions over all subjects in the subject space of the relation estimates the priors for the objects in the answer space.</p> <pre><code>import torch\n\nrelation_code = \"P30\"\n\nsoftmax = torch.nn.Softmax(dim=0)\nrelation_instance_table = bear_results[relation_code].instance_table\nrelation_instance_table[\"pll_softmax\"] = relation_instance_table.pll_scores.apply(lambda x: softmax(torch.tensor(x)))\nrelation_priors = pd.Series(\n    torch.mean(torch.stack(list(relation_instance_table.pll_softmax)), dim=0),\n    index=bear_results[relation_code].answer_space.values\n)\n</code></pre> <p>For the relation P30 this results in the following priors.</p> <pre><code>Africa           0.162270\nAntarctica       0.111288\nAsia             0.120442\nEurope           0.220671\nNorth America    0.218366\nSouth America    0.166962\ndtype: float64\n</code></pre> <p>We can see that gpt2 is biased towards answering \"North America\" and \"Europa\" when assessing the entities in the subject space of relation P30 with the template <code>[X] is located in [Y].</code> (though the objects are balanced).</p>"},{"location":"trainer_integration/","title":"Integration with Hugging Face's Trainer","text":"<p>LM Pub Quiz allows for easy integration of the BEAR probe with the Hugging Face Trainer. This way models can be continuously evaluated throughout training.</p>"},{"location":"trainer_integration/#lm-pub-quiz-callback","title":"LM Pub Quiz Callback","text":"<p>Here is an example of how to set up the Trainer Callback needed for integration.</p> <p><pre><code># import the PubQuiz Trainer Callback class\nfrom lm_pub_quiz.integrations.transformers_trainer import PubQuizCallback\n\n# set up the trainer as you usually would\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# load the BEAR dataset\nbear_dataset = Dataset.from_name(\"BEAR\")\n\n# create evaluator and Trainner callback\nevaluator = Evaluator.from_model(\n    model=model,\n    tokenizer=tokenizer,\n)\npub_quiz_callback = PubQuizCallback(\n    trainer=trainer,\n    evaluator=evaluator,\n    dataset=bear_dataset,\n    save_path=\"./bear_results\",\n)\n\n# add the PubQuiz callback to the Trainer\ntrainer.add_callback(pub_quiz_callback)\n\n# run training\ntrainer.train()\n</code></pre> Setting up the trainer prior to the callback is needed for the callback to have access to the logging functionality of the trainer. This way BEAR evaluation results are automatically logged by the trainer. And thus for example also included in a Tensorboard or Weights and Biases report.</p>"},{"location":"trainer_integration/#optional-parameters","title":"Optional Parameters","text":"<p>The LM Pub Quiz Callback performs the BEAR probe automatically, whenever the evaluation strategy of the Trainer calls for it. It's behaviour can be further customized with a number of optional parameters:</p> Argument Description <code>save_path</code> If other than <code>None</code>, full BEAR evaluation results are saved to this directory. <code>accumulate</code> one of [None, \"domains\", \"cardinality\"] <code>batch_size</code> specifies the batch size for the Evaluator <code>template</code> either specify the template index a pass a list of template indices"},{"location":"trainer_integration/#complete-example","title":"Complete Example","text":"<p>After the <code>training_run()</code> was completed you can view the reported metrics by calling <code>tensorboard --logdir logs/</code>. And inspect the full BEAR results saved at <code>&lt;PATH TO BEAR RESULTS SAVE DIR&gt;</code>. <pre><code>from datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForMaskedLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom lm_pub_quiz.integrations.transformers_trainer import PubQuizCallback\nfrom lm_pub_quiz.data import Dataset\nfrom lm_pub_quiz.evaluator import Evaluator\n\n\ndef training_run(\n        device=\"cpu\",\n        dataset_reduction_factor=0.002,\n):\n    # Load reduced dataset\n    dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n    select_train_indices = list(range(int(len(dataset['train']) * dataset_reduction_factor)))\n    select_validation_indices = list(range(int(len(dataset['validation']) * dataset_reduction_factor)))\n    dataset['train'] = dataset['train'].select(select_train_indices)\n    dataset['validation'] = dataset['validation'].select(select_validation_indices)\n\n    # load model and tokenizer\n    model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", truncation=True, max_length=512)\n\n    # do some minimal data preparation\n    def filter_text(example):\n        text = example['text']\n        if len(text) &lt; 100:\n            return False\n        if text[0] == '=' and text[-1] == '=':\n            return False\n        return True\n\n    dataset = dataset.filter(filter_text)\n\n    # tokenize data\n    def tokenize(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=512)\n\n    tokenized_train_ds = dataset['train'].map(tokenize, batched=True, remove_columns=['text'])\n    tokenized_eval_ds = dataset['validation'].map(tokenize, batched=True, remove_columns=['text'])\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n\n    # set up trainer with reporting to tensorboard\n    model_save_dir = './models/run_01'\n    training_args = TrainingArguments(\n        output_dir=model_save_dir,\n        run_name=\"run_01\",\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        learning_rate=5e-01,\n        lr_scheduler_type='constant',\n        eval_steps=5,\n        eval_strategy=\"steps\",\n        logging_steps=5,\n        logging_dir='logs',\n        report_to='tensorboard',\n        push_to_hub=False,\n        save_strategy=\"no\",\n        include_num_input_tokens_seen=True,\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_ds,\n        eval_dataset=tokenized_eval_ds,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # set up BEAR integration callback\n    dataset_path = \"&lt;PATH TO BEAR DATASET&gt;\"\n    bear_dataset = Dataset.from_path(dataset_path, relation_info=\"/home/seb/test_lm_pub_quiz/relation_info.json\")\n    bear_dataset = bear_dataset.filter_subset({\"P6\": list(range(5)), \"P30\": list(range(10)), \"P103\": list(range(5)), \"P175\": list(range(10))})\n    evaluator = Evaluator.from_model(\n        model=model,\n        tokenizer=tokenizer,\n        model_type=\"MLM\",\n        device=\"cpu\",\n    )\n    pub_quiz_callback = PubQuizCallback(\n        trainer=trainer,\n        evaluator=evaluator,\n        dataset=bear_dataset,\n        save_path=\"/home/seb/test_lm_pub_quiz/results\",\n        metrics=\"domains\",\n    )\n    trainer.add_callback(pub_quiz_callback)\n\n    # run training\n    trainer.train()\n    trainer.save_model()\n\ntraining_run()\n</code></pre></p>"},{"location":"api/base_classes/","title":"Data Base Clasess","text":"<p>The dataset representations as well as the evaluation results are based on common base classes.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase","title":"RelationBase","text":"<pre><code>RelationBase(\n    relation_code: str,\n    *,\n    lazy_options: Optional[dict[str, Any]] = None,\n    instance_table: Optional[DataFrame] = None,\n    answer_space: Optional[Series] = None,\n    relation_info: Optional[dict[str, Any]] = None\n)\n</code></pre> <p>Base class for the representation of relations and relations results.</p> <p>Methods:</p> Name Description <code>activated</code> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p> <code>copy</code> <p>Create a copy of the isntance with specified fields replaced by new values.</p> <code>from_path</code> <p>Load data from the given path.</p> <code>get_metadata</code> <p>Get or set metadata.</p> <code>relation_info</code> <p>Get or set additional relation information.</p> <code>save</code> <p>Save results to a file and export meta_data</p> <code>save_instance_table</code> <p>Save instance table with the format determined by the path suffix.</p> <code>search_path</code> <p>Search path for instance files.</p> <p>Attributes:</p> Name Type Description <code>answer_space</code> <code>Series</code> <p>The answer space of the relation.</p> <code>instance_table</code> <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p> <code>relation_code</code> <code>str</code> <p>The identifier of the relation.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.answer_space","title":"answer_space  <code>property</code>","text":"<pre><code>answer_space: Series\n</code></pre> <p>The answer space of the relation.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.instance_table","title":"instance_table  <code>property</code>","text":"<pre><code>instance_table: DataFrame\n</code></pre> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.relation_code","title":"relation_code  <code>property</code>","text":"<pre><code>relation_code: str\n</code></pre> <p>The identifier of the relation.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.activated","title":"activated","text":"<pre><code>activated() -&gt; Self\n</code></pre> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.copy","title":"copy","text":"<pre><code>copy(**kw)\n</code></pre> <p>Create a copy of the isntance with specified fields replaced by new values.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.from_path","title":"from_path  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None\n) -&gt; DataBase\n</code></pre> <p>Load data from the given path.</p> <p>If <code>lazy</code>, only the metadata is loaded and the instances are loaded once they are accessed.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata() -&gt; dict[str, Any]\n</code></pre><pre><code>get_metadata(key: str) -&gt; Any\n</code></pre> <pre><code>get_metadata(\n    key: Optional[str] = None,\n) -&gt; Union[Any, dict[str, Any]]\n</code></pre> <p>Get or set metadata.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>The metadata to retrieve. If not field is specified, the complete dictionary is returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Any, dict[str, Any]]</code> <p>Either the selected field or the complete dictionary.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.get_metadata(key)","title":"<code>key</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.relation_info","title":"relation_info","text":"<pre><code>relation_info(**kw) -&gt; dict[str, Any]\n</code></pre><pre><code>relation_info(key: str) -&gt; Any\n</code></pre> <pre><code>relation_info(\n    key: Optional[str] = None, /, **kw\n) -&gt; Union[None, Any, dict[str, Any]]\n</code></pre> <p>Get or set additional relation information.</p> <p>Use <code>relation.relation_info(&lt;field name&gt;=&lt;new value&gt;)</code> to set fields in the relation info dictionary. If a single field is selected, the respective value is returned. Otherwise the complete dictionary is returned.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>The field to retrieve.</p> <code>None</code> <p>The fields not modify.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, Any, dict[str, Any]]</code> <p>If a field is selected, the respective value is returned, otherwise, the complete info dictionary is</p> <code>Union[None, Any, dict[str, Any]]</code> <p>returned.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.relation_info(key)","title":"<code>key</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.relation_info(**kw)","title":"<code>**kw</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.save","title":"save","text":"<pre><code>save(\n    path: PathLike, fmt: InstanceTableFileFormat = None\n) -&gt; Optional[Path]\n</code></pre> <p>Save results to a file and export meta_data</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.save_instance_table","title":"save_instance_table  <code>classmethod</code>","text":"<pre><code>save_instance_table(\n    instance_table: DataFrame,\n    path: Path,\n    fmt: InstanceTableFileFormat = None,\n)\n</code></pre> <p>Save instance table with the format determined by the path suffix.</p> <p>Parameters:</p> Name Type Description Default <code>DataFrame</code> <p>The instances to save.</p> required <code>Path</code> <p>Where to save the instance table. If format is not specified, the suffix is used to determined           the format.</p> required <code>str</code> <p>Which to save the instances in.</p> <code>None</code>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.save_instance_table(instance_table)","title":"<code>instance_table</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.save_instance_table(path)","title":"<code>path</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.save_instance_table(fmt)","title":"<code>fmt</code>","text":""},{"location":"api/base_classes/#lm_pub_quiz.data.base.RelationBase.search_path","title":"search_path  <code>classmethod</code>","text":"<pre><code>search_path(\n    path: Path,\n    relation_code: None = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; list[Path]\n</code></pre><pre><code>search_path(\n    path: Path,\n    relation_code: str,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Path\n</code></pre> <pre><code>search_path(\n    path: Path,\n    relation_code: Optional[str] = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Union[list[Path], Path, None]\n</code></pre> <p>Search path for instance files.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.DatasetBase","title":"DatasetBase","text":"<p>Base class for a collection of relations or relations results.</p> <p>Methods:</p> Name Description <code>activated</code> <p>Return self if lazy loading is active, otherwise return a copy without lazy loading.</p> <code>from_path</code> <p>Load data from the given path.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.DatasetBase.activated","title":"activated  <code>abstractmethod</code>","text":"<pre><code>activated() -&gt; Self\n</code></pre> <p>Return self if lazy loading is active, otherwise return a copy without lazy loading.</p>"},{"location":"api/base_classes/#lm_pub_quiz.data.base.DatasetBase.from_path","title":"from_path  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None\n) -&gt; DataBase\n</code></pre> <p>Load data from the given path.</p> <p>If <code>lazy</code>, only the metadata is loaded and the instances are loaded once they are accessed.</p>"},{"location":"api/dataset/","title":"Dataset Representation","text":"<p>There are two classes which are used to represent a dataset: <code>Relation</code> and <code>Dataset</code> (which is essentially a container for a number of relations).</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation","title":"Relation","text":"<pre><code>Relation(\n    relation_code: str,\n    *,\n    templates: list[str],\n    answer_space: Optional[Series],\n    instance_table: Optional[DataFrame],\n    lazy_options: Optional[dict[str, Any]],\n    relation_info: Optional[dict[str, Any]] = None\n)\n</code></pre> <p>Represents a relation within a dataset, including its code, answer space, templates, and an instance table.</p> <p>Methods:</p> Name Description <code>activated</code> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p> <code>from_path</code> <p>Loads a relation from a JSONL file and associated metadata.</p> <code>relation_info</code> <p>Get or set additional relation information.</p> <code>save</code> <p>Save results to a file and export meta_data</p> <code>save_instance_table</code> <p>Save instance table with the format determined by the path suffix.</p> <code>search_path</code> <p>Search path for instance files.</p> <code>subsample</code> <p>Returns only a subsampled version of the dataset of the size n.</p> <p>Attributes:</p> Name Type Description <code>answer_space</code> <code>Series</code> <p>The answer space of the relation.</p> <code>instance_table</code> <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p> <code>relation_code</code> <code>str</code> <p>The identifier of the relation.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.answer_space","title":"answer_space  <code>property</code>","text":"<pre><code>answer_space: Series\n</code></pre> <p>The answer space of the relation.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.instance_table","title":"instance_table  <code>property</code>","text":"<pre><code>instance_table: DataFrame\n</code></pre> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.relation_code","title":"relation_code  <code>property</code>","text":"<pre><code>relation_code: str\n</code></pre> <p>The identifier of the relation.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.activated","title":"activated","text":"<pre><code>activated() -&gt; Self\n</code></pre> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    relation_code: Optional[str] = None,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None\n) -&gt; Self\n</code></pre> <p>Loads a relation from a JSONL file and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>PathLike</code> <p>The path to the dataset directory.</p> required <code>str</code> <p>The specific code of the relation to load.</p> <code>None</code> <code>bool</code> <p>If False, the instance table is loaded directly into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Relation</code> <code>Self</code> <p>An instance of the Relation class populated with data from the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the file or processing the data.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.from_path(path)","title":"<code>path</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.from_path(relation_code)","title":"<code>relation_code</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.from_path(lazy)","title":"<code>lazy</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.relation_info","title":"relation_info","text":"<pre><code>relation_info(**kw) -&gt; dict[str, Any]\n</code></pre><pre><code>relation_info(key: str) -&gt; Any\n</code></pre> <pre><code>relation_info(\n    key: Optional[str] = None, /, **kw\n) -&gt; Union[None, Any, dict[str, Any]]\n</code></pre> <p>Get or set additional relation information.</p> <p>Use <code>relation.relation_info(&lt;field name&gt;=&lt;new value&gt;)</code> to set fields in the relation info dictionary. If a single field is selected, the respective value is returned. Otherwise the complete dictionary is returned.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>The field to retrieve.</p> <code>None</code> <p>The fields not modify.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, Any, dict[str, Any]]</code> <p>If a field is selected, the respective value is returned, otherwise, the complete info dictionary is</p> <code>Union[None, Any, dict[str, Any]]</code> <p>returned.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.relation_info(key)","title":"<code>key</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.relation_info(**kw)","title":"<code>**kw</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.save","title":"save","text":"<pre><code>save(\n    path: PathLike, fmt: InstanceTableFileFormat = None\n) -&gt; Optional[Path]\n</code></pre> <p>Save results to a file and export meta_data</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.save_instance_table","title":"save_instance_table  <code>classmethod</code>","text":"<pre><code>save_instance_table(\n    instance_table: DataFrame,\n    path: Path,\n    fmt: InstanceTableFileFormat = None,\n)\n</code></pre> <p>Save instance table with the format determined by the path suffix.</p> <p>Parameters:</p> Name Type Description Default <code>DataFrame</code> <p>The instances to save.</p> required <code>Path</code> <p>Where to save the instance table. If format is not specified, the suffix is used to determined           the format.</p> required <code>str</code> <p>Which to save the instances in.</p> <code>None</code>"},{"location":"api/dataset/#lm_pub_quiz.Relation.save_instance_table(instance_table)","title":"<code>instance_table</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.save_instance_table(path)","title":"<code>path</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.save_instance_table(fmt)","title":"<code>fmt</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Relation.search_path","title":"search_path  <code>classmethod</code>","text":"<pre><code>search_path(\n    path: Path,\n    relation_code: None = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; list[Path]\n</code></pre><pre><code>search_path(\n    path: Path,\n    relation_code: str,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Path\n</code></pre> <pre><code>search_path(\n    path: Path,\n    relation_code: Optional[str] = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Union[list[Path], Path, None]\n</code></pre> <p>Search path for instance files.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.subsample","title":"subsample","text":"<pre><code>subsample(n: int = 10) -&gt; DataFrame\n</code></pre> <p>Returns only a subsampled version of the dataset of the size n.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Size of the subsampled dataset</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Subsampled version of the dataset.</p>"},{"location":"api/dataset/#lm_pub_quiz.Relation.subsample(n)","title":"<code>n</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Dataset","title":"Dataset","text":"<pre><code>Dataset(\n    relations: list[Relation],\n    path: PathLike,\n    name: Optional[str] = None,\n)\n</code></pre> <p>A collection of relations forming a multiple choice dataset.</p> Usage <p>The prefferred way to load the BEAR knowledge probe is to load it by name:</p> <p>from lm_pub_quiz import Dataset dataset = Dataset.from_name(\"BEAR\")</p> <p>Methods:</p> Name Description <code>from_name</code> <p>Loads a dataset from the cache (if available) or the url which is specified in the internal dataset table.</p> <code>from_path</code> <p>Loads a multiple choice dataset from a specified directory path.</p>"},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_name","title":"from_name  <code>classmethod</code>","text":"<pre><code>from_name(\n    name: str,\n    *,\n    lazy: bool = True,\n    base_path: Optional[Path] = None,\n    chunk_size: int = 10 * 1024,\n    relation_info: Optional[PathLike] = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Loads a dataset from the cache (if available) or the url which is specified in the internal dataset table.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the dataset.</p> required <code>bool</code> <p>If False, the instance tables of all relations are directly loaded into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Self</code> <p>An instance if Dataset loaded with the relations from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading the BEAR-dataset. <pre><code>&gt;&gt;&gt; from lm_pub_quiz import Dataset\n&gt;&gt;&gt; dataset = Dataset.from_name(\"BEAR\")\n</code></pre></p>"},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_name(name)","title":"<code>name</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_name(lazy)","title":"<code>lazy</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None,\n    relation_info: Optional[PathLike] = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Loads a multiple choice dataset from a specified directory path.</p> <p>This method scans the directory for relation files and assembles them into a MultipleChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The directory path where the dataset is stored.</p> required <code>bool</code> <p>If False, the instance tables of all relations are directly loaded into memory.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Self</code> <p>An instance if Dataset loaded with the relations from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading the BEAR-dataset. <pre><code>&gt;&gt;&gt; from lm_pub_quiz import Dataset\n&gt;&gt;&gt; dataset = Dataset.from_path(\"/path/to/dataset/BEAR\")\n</code></pre></p>"},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_path(path)","title":"<code>path</code>","text":""},{"location":"api/dataset/#lm_pub_quiz.Dataset.from_path(lazy)","title":"<code>lazy</code>","text":""},{"location":"api/evaluator/","title":"Evaluator Classes","text":"<p>The (pseudo) log-likelihood-based approaches derive from the <code>Evaluator</code> class which implements a lot of the basic functionality. To create an evaluator instance, use <code>Evaluator.from_model</code>.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(*, conditional_score: bool = False, **kwargs)\n</code></pre> <p>Base class for PLL-based evaluation classes.</p> <p>Use <code>Evaluator.from_model</code> to create a suitable model-specific <code>Evaluator</code> instance.</p> <p>Methods:</p> Name Description <code>encode</code> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <code>evaluate_dataset</code> <p>Evaluate the model on all relations in the dataset.</p> <code>from_model</code> <p>Create an evaluator instance for the given model.</p> <code>replace_placeholders</code> <p>Replace all placeholders in the template with the respective values.</p> <code>score_answers</code> <p>Calculate sequence scores using the Casual Language Model.</p> <code>score_statements</code> <p>Compute the PLL score for the tokens (determined by the scoring mask) in a statements.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(\n    statements: Sequence[str],\n    span_roles: Sequence[SpanRoles],\n) -&gt; tuple[BatchEncoding, Sequence[ScoringMask]]\n</code></pre> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <p>In case the conditional scores need to be created, set the scoring mask accordingly.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.evaluate_dataset","title":"evaluate_dataset","text":"<pre><code>evaluate_dataset(\n    dataset: Dataset,\n    template_index: int = 0,\n    *,\n    batch_size: int = 1,\n    subsample: Optional[int] = None,\n    save_path: Optional[PathLike] = None,\n    fmt: InstanceTableFileFormat = None,\n    reduction: Optional[str] = \"default\",\n    create_instance_table: bool = True,\n    metric: Optional[MultiMetricSpecification] = None\n) -&gt; DatasetResults\n</code></pre> <p>Evaluate the model on all relations in the dataset.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.from_model","title":"from_model  <code>classmethod</code>","text":"<pre><code>from_model(\n    model: Union[str, PreTrainedModel],\n    model_type: Optional[str] = None,\n    **kw,\n) -&gt; Evaluator\n</code></pre> <p>Create an evaluator instance for the given model.</p> <p>In some cases, the model type can be derived from the model itself. To ensure the right type is chosen, it's recommended to set <code>model_type</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedModel</code> <p>The model to evaluate.</p> required <code>str | None</code> <p>The type of model (determines the scoring scheme to be used).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Evaluator</code> <code>Evaluator</code> <p>The evaluator instance suitable for the model.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.from_model(model)","title":"<code>model</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.from_model(model_type)","title":"<code>model_type</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.replace_placeholders","title":"replace_placeholders","text":"<pre><code>replace_placeholders(\n    *,\n    template: str,\n    subject: Optional[str],\n    answer: Optional[str]\n) -&gt; tuple[str, SpanRoles]\n</code></pre> <p>Replace all placeholders in the template with the respective values.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The temaplate string with appropriate placeholders.</p> required <code>Optional[str]</code> <p>The subject label to fill in at the resective placeholder.</p> required <code>Optional[str]</code> <p>The answer span to fill in.</p> required <p>Returns:</p> Type Description <code>tuple[str, SpanRoles]</code> <p>The final string as well as the spans of the respective elements in the final string.</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.replace_placeholders(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.replace_placeholders(subject)","title":"<code>subject</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.replace_placeholders(answer)","title":"<code>answer</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.score_answers","title":"score_answers","text":"<pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: None,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; EachTokenReturnFormat\n</code></pre><pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: str,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; ReducedReturnFormat\n</code></pre> <pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: Optional[str],\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; Union[EachTokenReturnFormat, ReducedReturnFormat]\n</code></pre> <p>Calculate sequence scores using the Casual Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The template to use (should contain a <code>[Y]</code> marker).</p> required <code>list[str]</code> <p>List of answers to calculate score for.</p> required <p>Returns:</p> Type Description <code>Union[EachTokenReturnFormat, ReducedReturnFormat]</code> <p>list[float]: List of suprisals scores per sequence</p>"},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.score_answers(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.score_answers(answers)","title":"<code>answers</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.Evaluator.score_statements","title":"score_statements  <code>abstractmethod</code>","text":"<pre><code>score_statements(\n    batched_statements: BatchEncoding,\n    *,\n    scoring_masks: Optional[Sequence[ScoringMask]],\n    batch_size: int = 1\n) -&gt; list[list[float]]\n</code></pre> <p>Compute the PLL score for the tokens (determined by the scoring mask) in a statements.</p> <p>This function must be implemented by child-classes for each model-type.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator","title":"MaskedLMEvaluator","text":"<pre><code>MaskedLMEvaluator(\n    *, pll_metric: str = \"within_word_l2r\", **kw\n)\n</code></pre> <p>Methods:</p> Name Description <code>create_masked_batch</code> <p>Extend the existing batch and mask the relevant tokens based on the scoring mask.</p> <code>encode</code> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <code>evaluate_dataset</code> <p>Evaluate the model on all relations in the dataset.</p> <code>from_model</code> <p>Create an evaluator instance for the given model.</p> <code>mask_to_indices</code> <p>Transform the scoring mask to a list of indices.</p> <code>replace_placeholders</code> <p>Replace all placeholders in the template with the respective values.</p> <code>score_answers</code> <p>Calculate sequence scores using the Casual Language Model.</p> <p>Attributes:</p> Name Type Description <code>mask_token</code> <code>int</code> <p>Return the mask token id used by the tokenizer.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.mask_token","title":"mask_token  <code>property</code>","text":"<pre><code>mask_token: int\n</code></pre> <p>Return the mask token id used by the tokenizer.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.create_masked_batch","title":"create_masked_batch","text":"<pre><code>create_masked_batch(\n    batch: BatchEncoding,\n    scoring_masks: Sequence[ScoringMask],\n) -&gt; BatchEncoding\n</code></pre> <p>Extend the existing batch and mask the relevant tokens based on the scoring mask.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.encode","title":"encode","text":"<pre><code>encode(\n    statements: Sequence[str],\n    span_roles: Sequence[SpanRoles],\n) -&gt; tuple[BatchEncoding, Sequence[ScoringMask]]\n</code></pre> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <p>In case the conditional scores need to be created, set the scoring mask accordingly.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.evaluate_dataset","title":"evaluate_dataset","text":"<pre><code>evaluate_dataset(\n    dataset: Dataset,\n    template_index: int = 0,\n    *,\n    batch_size: int = 1,\n    subsample: Optional[int] = None,\n    save_path: Optional[PathLike] = None,\n    fmt: InstanceTableFileFormat = None,\n    reduction: Optional[str] = \"default\",\n    create_instance_table: bool = True,\n    metric: Optional[MultiMetricSpecification] = None\n) -&gt; DatasetResults\n</code></pre> <p>Evaluate the model on all relations in the dataset.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.from_model","title":"from_model  <code>classmethod</code>","text":"<pre><code>from_model(\n    model: Union[str, PreTrainedModel],\n    model_type: Optional[str] = None,\n    **kw,\n) -&gt; Evaluator\n</code></pre> <p>Create an evaluator instance for the given model.</p> <p>In some cases, the model type can be derived from the model itself. To ensure the right type is chosen, it's recommended to set <code>model_type</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedModel</code> <p>The model to evaluate.</p> required <code>str | None</code> <p>The type of model (determines the scoring scheme to be used).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Evaluator</code> <code>Evaluator</code> <p>The evaluator instance suitable for the model.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.from_model(model)","title":"<code>model</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.from_model(model_type)","title":"<code>model_type</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.mask_to_indices","title":"mask_to_indices","text":"<pre><code>mask_to_indices(\n    scoring_masks: Sequence[ScoringMask],\n) -&gt; list[Tensor]\n</code></pre> <p>Transform the scoring mask to a list of indices.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.replace_placeholders","title":"replace_placeholders","text":"<pre><code>replace_placeholders(\n    *,\n    template: str,\n    subject: Optional[str],\n    answer: Optional[str]\n) -&gt; tuple[str, SpanRoles]\n</code></pre> <p>Replace all placeholders in the template with the respective values.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The temaplate string with appropriate placeholders.</p> required <code>Optional[str]</code> <p>The subject label to fill in at the resective placeholder.</p> required <code>Optional[str]</code> <p>The answer span to fill in.</p> required <p>Returns:</p> Type Description <code>tuple[str, SpanRoles]</code> <p>The final string as well as the spans of the respective elements in the final string.</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.replace_placeholders(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.replace_placeholders(subject)","title":"<code>subject</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.replace_placeholders(answer)","title":"<code>answer</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.score_answers","title":"score_answers","text":"<pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: None,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; EachTokenReturnFormat\n</code></pre><pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: str,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; ReducedReturnFormat\n</code></pre> <pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: Optional[str],\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; Union[EachTokenReturnFormat, ReducedReturnFormat]\n</code></pre> <p>Calculate sequence scores using the Casual Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The template to use (should contain a <code>[Y]</code> marker).</p> required <code>list[str]</code> <p>List of answers to calculate score for.</p> required <p>Returns:</p> Type Description <code>Union[EachTokenReturnFormat, ReducedReturnFormat]</code> <p>list[float]: List of suprisals scores per sequence</p>"},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.score_answers(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.MaskedLMEvaluator.score_answers(answers)","title":"<code>answers</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator","title":"CausalLMEvaluator","text":"<pre><code>CausalLMEvaluator(\n    *, conditional_score: bool = False, **kwargs\n)\n</code></pre> <p>Methods:</p> Name Description <code>encode</code> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <code>evaluate_dataset</code> <p>Evaluate the model on all relations in the dataset.</p> <code>from_model</code> <p>Create an evaluator instance for the given model.</p> <code>replace_placeholders</code> <p>Replace all placeholders in the template with the respective values.</p> <code>score_answers</code> <p>Calculate sequence scores using the Casual Language Model.</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.encode","title":"encode","text":"<pre><code>encode(\n    statements: Sequence[str],\n    span_roles: Sequence[SpanRoles],\n) -&gt; tuple[BatchEncoding, Sequence[ScoringMask]]\n</code></pre> <p>Encode the statements using the tokenizer and create an appropriate scoring mask.</p> <p>In case the conditional scores need to be created, set the scoring mask accordingly.</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.evaluate_dataset","title":"evaluate_dataset","text":"<pre><code>evaluate_dataset(\n    dataset: Dataset,\n    template_index: int = 0,\n    *,\n    batch_size: int = 1,\n    subsample: Optional[int] = None,\n    save_path: Optional[PathLike] = None,\n    fmt: InstanceTableFileFormat = None,\n    reduction: Optional[str] = \"default\",\n    create_instance_table: bool = True,\n    metric: Optional[MultiMetricSpecification] = None\n) -&gt; DatasetResults\n</code></pre> <p>Evaluate the model on all relations in the dataset.</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.from_model","title":"from_model  <code>classmethod</code>","text":"<pre><code>from_model(\n    model: Union[str, PreTrainedModel],\n    model_type: Optional[str] = None,\n    **kw,\n) -&gt; Evaluator\n</code></pre> <p>Create an evaluator instance for the given model.</p> <p>In some cases, the model type can be derived from the model itself. To ensure the right type is chosen, it's recommended to set <code>model_type</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedModel</code> <p>The model to evaluate.</p> required <code>str | None</code> <p>The type of model (determines the scoring scheme to be used).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Evaluator</code> <code>Evaluator</code> <p>The evaluator instance suitable for the model.</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.from_model(model)","title":"<code>model</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.from_model(model_type)","title":"<code>model_type</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.replace_placeholders","title":"replace_placeholders","text":"<pre><code>replace_placeholders(\n    *,\n    template: str,\n    subject: Optional[str],\n    answer: Optional[str]\n) -&gt; tuple[str, SpanRoles]\n</code></pre> <p>Replace all placeholders in the template with the respective values.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The temaplate string with appropriate placeholders.</p> required <code>Optional[str]</code> <p>The subject label to fill in at the resective placeholder.</p> required <code>Optional[str]</code> <p>The answer span to fill in.</p> required <p>Returns:</p> Type Description <code>tuple[str, SpanRoles]</code> <p>The final string as well as the spans of the respective elements in the final string.</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.replace_placeholders(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.replace_placeholders(subject)","title":"<code>subject</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.replace_placeholders(answer)","title":"<code>answer</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.score_answers","title":"score_answers","text":"<pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: None,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; EachTokenReturnFormat\n</code></pre><pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: str,\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; ReducedReturnFormat\n</code></pre> <pre><code>score_answers(\n    *,\n    template: str,\n    answers: Sequence[str],\n    reduction: Optional[str],\n    subject: Optional[str] = None,\n    batch_size: int = 1\n) -&gt; Union[EachTokenReturnFormat, ReducedReturnFormat]\n</code></pre> <p>Calculate sequence scores using the Casual Language Model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The template to use (should contain a <code>[Y]</code> marker).</p> required <code>list[str]</code> <p>List of answers to calculate score for.</p> required <p>Returns:</p> Type Description <code>Union[EachTokenReturnFormat, ReducedReturnFormat]</code> <p>list[float]: List of suprisals scores per sequence</p>"},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.score_answers(template)","title":"<code>template</code>","text":""},{"location":"api/evaluator/#lm_pub_quiz.CausalLMEvaluator.score_answers(answers)","title":"<code>answers</code>","text":""},{"location":"api/result/","title":"Evaluation Result","text":"<p>Similar to the dataset representation, the results are also represented in two classes <code>RelationResult</code> and the container <code>DatasetResults</code>.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult","title":"RelationResult","text":"<pre><code>RelationResult(\n    relation_code: str,\n    *,\n    metadata: dict[str, Any],\n    metric_values: Optional[dict[str, Any]] = None,\n    instance_table: Optional[DataFrame] = None,\n    answer_space: Optional[Series] = None,\n    lazy_options: Optional[dict[str, Any]] = None,\n    relation_info: Optional[dict[str, Any]] = None\n)\n</code></pre> <p>Methods:</p> Name Description <code>activated</code> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p> <code>from_path</code> <p>Loads the evaluated relation from a JSONL file and associated metadata.</p> <code>relation_info</code> <p>Get or set additional relation information.</p> <code>save</code> <p>Save results to a file and export meta_data</p> <code>search_path</code> <p>Search path for instance files.</p> <p>Attributes:</p> Name Type Description <code>answer_space</code> <code>Series</code> <p>The answer space of the relation.</p> <code>instance_table</code> <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p> <code>relation_code</code> <code>str</code> <p>The identifier of the relation.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.answer_space","title":"answer_space  <code>property</code>","text":"<pre><code>answer_space: Series\n</code></pre> <p>The answer space of the relation.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.instance_table","title":"instance_table  <code>property</code>","text":"<pre><code>instance_table: DataFrame\n</code></pre> <p>A <code>pandas.DataFrame</code> containing all items in the relation.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.relation_code","title":"relation_code  <code>property</code>","text":"<pre><code>relation_code: str\n</code></pre> <p>The identifier of the relation.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.activated","title":"activated","text":"<pre><code>activated() -&gt; Self\n</code></pre> <p>Return self or a copy of self with the instance_table loaded (lazy loading disabled).</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    relation_code: Optional[str] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None\n) -&gt; RelationResult\n</code></pre> <p>Loads the evaluated relation from a JSONL file and associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>PathLike</code> <p>The path to the relations instance table.</p> required <p>Returns:</p> Type Description <code>RelationResult</code> <p>An instance of the RelationResult class populated with data from the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the file or processing the data.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.from_path(path)","title":"<code>path</code>","text":""},{"location":"api/result/#lm_pub_quiz.RelationResult.relation_info","title":"relation_info","text":"<pre><code>relation_info(**kw) -&gt; dict[str, Any]\n</code></pre><pre><code>relation_info(key: str) -&gt; Any\n</code></pre> <pre><code>relation_info(\n    key: Optional[str] = None, /, **kw\n) -&gt; Union[None, Any, dict[str, Any]]\n</code></pre> <p>Get or set additional relation information.</p> <p>Use <code>relation.relation_info(&lt;field name&gt;=&lt;new value&gt;)</code> to set fields in the relation info dictionary. If a single field is selected, the respective value is returned. Otherwise the complete dictionary is returned.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>The field to retrieve.</p> <code>None</code> <p>The fields not modify.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, Any, dict[str, Any]]</code> <p>If a field is selected, the respective value is returned, otherwise, the complete info dictionary is</p> <code>Union[None, Any, dict[str, Any]]</code> <p>returned.</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.relation_info(key)","title":"<code>key</code>","text":""},{"location":"api/result/#lm_pub_quiz.RelationResult.relation_info(**kw)","title":"<code>**kw</code>","text":""},{"location":"api/result/#lm_pub_quiz.RelationResult.save","title":"save","text":"<pre><code>save(\n    path: PathLike, fmt: InstanceTableFileFormat = None\n) -&gt; Optional[Path]\n</code></pre> <p>Save results to a file and export meta_data</p>"},{"location":"api/result/#lm_pub_quiz.RelationResult.search_path","title":"search_path  <code>classmethod</code>","text":"<pre><code>search_path(\n    path: Path,\n    relation_code: None = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; list[Path]\n</code></pre><pre><code>search_path(\n    path: Path,\n    relation_code: str,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Path\n</code></pre> <pre><code>search_path(\n    path: Path,\n    relation_code: Optional[str] = None,\n    fmt: InstanceTableFileFormat = None,\n) -&gt; Union[list[Path], Path, None]\n</code></pre> <p>Search path for instance files.</p>"},{"location":"api/result/#lm_pub_quiz.DatasetResults","title":"DatasetResults","text":"<pre><code>DatasetResults(\n    results: Optional[list[RelationResult]] = None,\n)\n</code></pre> <p>Container for relation results.</p> <p>Methods:</p> Name Description <code>from_path</code> <p>Loads a results from a specified directory path.</p> <code>get_metadata</code> <p>Return metadata from the relations. If no keys are passed, all consistent values are returned.</p> <code>get_metrics</code> <p>Return the metrics for the relations in this dataset.</p>"},{"location":"api/result/#lm_pub_quiz.DatasetResults.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(\n    path: PathLike,\n    *,\n    lazy: bool = True,\n    fmt: InstanceTableFileFormat = None,\n    relation_info: Optional[PathLike] = None,\n    **kwargs\n) -&gt; DatasetResults\n</code></pre> <p>Loads a results from a specified directory path.</p> <p>This method scans the directory for relation files and assembles them into a DatasetResults.</p> <p>Parameters:</p> Name Type Description Default <code>PathLike</code> <p>The directory path where the dataset is stored.</p> required <p>Returns:</p> Type Description <code>DatasetResults</code> <p>An instance of DatasetResults loaded with the results from the directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error in loading the dataset.</p> Usage <p>Loading all relation results for a dataset. <pre><code>from results import DatasetResults\nresults = DatasetResults.load_from_path('/path/to/results/', dataset_name='BEAR')\n</code></pre></p>"},{"location":"api/result/#lm_pub_quiz.DatasetResults.from_path(path)","title":"<code>path</code>","text":""},{"location":"api/result/#lm_pub_quiz.DatasetResults.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(key: None = None) -&gt; dict[str, Any]\n</code></pre><pre><code>get_metadata(key: str) -&gt; Any\n</code></pre><pre><code>get_metadata(key: list[str]) -&gt; dict[str, Any]\n</code></pre> <pre><code>get_metadata(key: Optional[Union[str, list[str]]] = None)\n</code></pre> <p>Return metadata from the relations. If no keys are passed, all consistent values are returned.</p>"},{"location":"api/result/#lm_pub_quiz.DatasetResults.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics(\n    metrics: str,\n    *,\n    accumulate: Literal[True] = True,\n    divide_support: bool = True\n) -&gt; float\n</code></pre><pre><code>get_metrics(\n    metrics: str,\n    *,\n    accumulate: Union[Literal[False], None, str],\n    divide_support: bool = True\n) -&gt; Series\n</code></pre><pre><code>get_metrics(\n    metrics: Sequence[str],\n    *,\n    accumulate: Literal[True] = True,\n    divide_support: bool = True\n) -&gt; Series\n</code></pre><pre><code>get_metrics(\n    metrics: Sequence[str],\n    *,\n    accumulate: Union[Literal[False], None, str],\n    divide_support: bool = True\n) -&gt; DataFrame\n</code></pre> <pre><code>get_metrics(\n    metrics: Union[str, Sequence[str]],\n    *,\n    accumulate: Union[bool, None, str] = True,\n    divide_support: bool = True\n) -&gt; Union[DataFrame, Series, float]\n</code></pre> <p>Return the metrics for the relations in this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bool | str | None</code> <p>Compute the metrics for groups of relations (e.g. over the domains) or compute the overall scores for the complete dataset by setting <code>accumulate=True</code>.</p> <code>True</code> <code>bool</code> <p>Set to true to divide the support (added by a relation to a group) by the number of groups it adds to (only relevant if there are multiple groups per relation i.e. when <code>explode</code> is set). This leads to a dataframe where the weightted mean is equal to the overall score.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Series, float]</code> <p>pandas.DataFrame | pandas.Series: A Series or DataFrame with the selected metrics depending on whether all relations where accumulated.</p>"},{"location":"api/result/#lm_pub_quiz.DatasetResults.get_metrics(accumulate)","title":"<code>accumulate</code>","text":""},{"location":"api/result/#lm_pub_quiz.DatasetResults.get_metrics(divide_support)","title":"<code>divide_support</code>","text":""}]}